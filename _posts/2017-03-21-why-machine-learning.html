---
layout: post
title: Why machine learning
tagline: " is unavoidable?"
permalink: /machine-learning/neural-network/why.html
categories: [machine learning, neural networks]
tags: [definition, introduction]
---

<div id="table-of-contents">
<h2>Table of Contents</h2>
<div id="text-table-of-contents">
<ul>
<li><a href="#org92a3f7e">What is Machine Learning?</a>
<ul>
<li><a href="#org74de5db">The Machine Learning Approach</a></li>
<li><a href="#org7c92351">Some examples of tasks best solved by learning</a></li>
<li><a href="#orgbdabca2">A standard example of machine learning</a></li>
<li><a href="#org2fdca16">It is very hard to say what makes a 2</a></li>
<li><a href="#org2dd51b2">Beyond MNIST: The ImageNet task</a></li>
<li><a href="#org290cd87">Some examples from an earlier version of the net</a></li>
<li><a href="#orgaa40dac">It can deal with a wide range of objects</a></li>
<li><a href="#org14595ae">It makes some really cool errors</a></li>
<li><a href="#orgbf2ca55">The Speech Recognition Task</a></li>
<li><a href="#org812fcc5">Phone recognition on the TIMIT benchmark</a></li>
<li><a href="#org0d184d5">Word error rates from MSR, IBM, &amp; Google</a></li>
<li><a href="#orgd5587d7">Transcript</a></li>
</ul>
</li>
<li><a href="#orge5bad48">What are neural networks?</a>
<ul>
<li><a href="#orge893fe5">Reasons to study neural computation</a></li>
<li><a href="#org4b90c43">A typical cortical neuron</a></li>
<li><a href="#orgae27dfa">Synapses</a></li>
<li><a href="#orgaf2a77c">How synapses adapt</a></li>
<li><a href="#orge9999d0">How the brain works on one slide!</a></li>
<li><a href="#org1c97488">Modularity and the brain</a></li>
</ul>
</li>
<li><a href="#org5c54bce">Some simple models of neurons</a>
<ul>
<li><a href="#org80167bc">Idealized neurons</a></li>
</ul>
</li>
</ul>
</div>
</div>
<blockquote>
<p>
Why do we need machine learning?
</p>
</blockquote>

<div id="outline-container-org92a3f7e" class="outline-2">
<h2 id="org92a3f7e">What is Machine Learning?</h2>
<div class="outline-text-2" id="text-org92a3f7e">

<div class="figure">
<p><img src="https://dmm613.files.wordpress.com/2014/09/figure1.png" alt="Different approach" width="30%" title="ML concept" align="right" />
</p>
<p><span class="figure-number">Figure 1: </span>ML distinction</p>
</div>


<p>
It is very hard to write programs that solve problems like recognizing a
three-dimensional object from a novel viewpoint in new lighting
conditions in a cluttered scene.
</p>

<ul class="org-ul">
<li>We don’t know what program to write because we don’t know how its
done in our brain.</li>
<li>Even if we had a good idea about how to do it, the program might
be horrendously complicated.</li>
</ul>

<p>
It is hard to write a program to compute the probability that a
credit card transaction is fraudulent.
</p>
<ul class="org-ul">
<li>There may not be any rules that are both simple and reliable. We
need to combine a very large number of weak rules.</li>
<li>Fraud is a moving target. The program needs to keep changing.</li>
</ul>
</div>

<div id="outline-container-org74de5db" class="outline-3">
<h3 id="org74de5db">The Machine Learning Approach</h3>
<div class="outline-text-3" id="text-org74de5db">
<p>
Instead of writing a program by hand for each specific task, we collect
lots of examples that specify the correct output for a given input.
A machine learning algorithm then takes these examples and produces
a program that does the job.
</p>

<ul class="org-ul">
<li>The program produced by the learning algorithm may look very
different from a typical hand-written program. It may contain
millions of numbers.</li>
<li>If we do it right, the program works for new cases as well as the
ones we trained it on.</li>
<li>If the data changes the program can change too by training on the
new data.</li>
</ul>

<p>
Massive amounts of computation are now cheaper than paying
someone to write a task-specific program.
</p>
</div>
</div>

<div id="outline-container-org7c92351" class="outline-3">
<h3 id="org7c92351">Some examples of tasks best solved by learning</h3>
<div class="outline-text-3" id="text-org7c92351">
</div><div id="outline-container-orgf926c3b" class="outline-4">
<h4 id="orgf926c3b">Recognizing patterns:</h4>
<div class="outline-text-4" id="text-orgf926c3b">
<ul class="org-ul">
<li>Objects in real scenes</li>
<li>Facial identities or facial expressions</li>
<li>Spoken words</li>
</ul>
</div>
</div>

<div id="outline-container-orga7f549b" class="outline-4">
<h4 id="orga7f549b">Recognizing anomalies:</h4>
<div class="outline-text-4" id="text-orga7f549b">
<ul class="org-ul">
<li>Unusual sequences of credit card transactions</li>
<li>Unusual patterns of sensor readings in a nuclear power plant</li>
</ul>
</div>
</div>

<div id="outline-container-org3d7642e" class="outline-4">
<h4 id="org3d7642e">Prediction:</h4>
<div class="outline-text-4" id="text-org3d7642e">
<ul class="org-ul">
<li>Future stock prices or currency exchange rates</li>
<li>Which movies will a person like?</li>
</ul>
</div>
</div>
</div>

<div id="outline-container-orgbdabca2" class="outline-3">
<h3 id="orgbdabca2">A standard example of machine learning</h3>
<div class="outline-text-3" id="text-orgbdabca2">
<p>
A lot of genetics is done on fruit flies.
</p>

<ul class="org-ul">
<li>They are convenient because they breed fast.</li>
<li>We already know a lot about them.</li>
</ul>

<p>
The MNIST database of hand-written digits is the the machine
learning equivalent of fruit flies.
</p>

<ul class="org-ul">
<li>They are publicly available and we can learn them quite fast in a
moderate-sized neural net.</li>
<li>We know a huge amount about how well various machine learning
methods do on MNIST.</li>
</ul>

<p>
We will use MNIST as our standard task.
</p>
</div>
</div>

<div id="outline-container-org2fdca16" class="outline-3">
<h3 id="org2fdca16">It is very hard to say what makes a 2</h3>
</div>

<div id="outline-container-org2dd51b2" class="outline-3">
<h3 id="org2dd51b2">Beyond MNIST: The ImageNet task</h3>
<div class="outline-text-3" id="text-org2dd51b2">
<p>
1000 different object classes in 1.3 million high-resolution training images
from the web.
</p>

<ul class="org-ul">
<li>Best system in 2010 competition got 47% error for its first choice
and 25% error for its top 5 choices.</li>
</ul>

<p>
Jitendra Malik (an eminent neural net sceptic) said that this competition is
a good test of whether deep neural networks work well for object
recognition.
</p>

<ul class="org-ul">
<li>A very deep neural net (Krizhevsky et. al. 2012) gets less that
40% error for its first choice and less than 20% for its top 5
choices (see lecture 5).</li>
</ul>
</div>
</div>

<div id="outline-container-org290cd87" class="outline-3">
<h3 id="org290cd87">Some examples from an earlier version of the net</h3>
</div>

<div id="outline-container-orgaa40dac" class="outline-3">
<h3 id="orgaa40dac">It can deal with a wide range of objects</h3>
</div>

<div id="outline-container-org14595ae" class="outline-3">
<h3 id="org14595ae">It makes some really cool errors</h3>
</div>

<div id="outline-container-orgbf2ca55" class="outline-3">
<h3 id="orgbf2ca55">The Speech Recognition Task</h3>
<div class="outline-text-3" id="text-orgbf2ca55">
<p>
A speech recognition system has several stages:
</p>

<ul class="org-ul">
<li><i>Pre-processing</i>: Convert the sound wave into a vector of acoustic
coefficients. Extract a new vector about every 10 mille seconds.</li>
<li><i>The acoustic model</i>: Use a few adjacent vectors of acoustic
coefficients to place bets on which part of which phoneme is being
spoken.</li>
<li><i>Decoding</i>: Find the sequence of bets that does the best job of
fitting the acoustic data and also fitting a model of the kinds of
things people say.</li>
</ul>

<p>
Deep neural networks pioneered by George Dahl and Abdel-rahman
Mohamed are now replacing the previous machine learning method
for the acoustic model.
</p>
</div>
</div>

<div id="outline-container-org812fcc5" class="outline-3">
<h3 id="org812fcc5">Phone recognition on the TIMIT benchmark</h3>
<div class="outline-text-3" id="text-org812fcc5">
<p>
(Mohamed, Dahl, &amp; Hinton, 2012)
183 HMM-state labels
not pre-trained
2000 logistic hidden units
5 more layers of
pre-trained weights
2000 logistic hidden units
2000 logistic hidden units
15 frames of 40 filterbank outputs
</p>
<ul class="org-ul">
<li>their temporal derivatives</li>
</ul>


<ul class="org-ul">
<li>After standard post-processing using a bi-phone model, a deep
net with 8 layers gets 20.7% error rate.</li>
<li>The best previous speaker- independent result on TIMIT was 24.4%
and this required averaging several models.</li>
<li>Li Deng (at MSR) realised that this result could change the way
speech recognition was done.</li>
</ul>
</div>
</div>

<div id="outline-container-org0d184d5" class="outline-3">
<h3 id="org0d184d5">Word error rates from MSR, IBM, &amp; Google</h3>
<div class="outline-text-3" id="text-org0d184d5">
<p>
(Hinton et. al. IEEE Signal Processing Magazine, Nov 2012)
</p>

<p>
The task Hours of
training data Deep neural
network Gaussian
Mixture
Model GMM with
more data
Switchboard
(Microsoft
Research) 309 18.5% 27.4% 18.6%
(2000 hrs)
English broadcast
news (IBM) 50 17.5% 18.8% Google voice
search
(android 4.1) 5,870 12.3%
(and falling)
16.0%
(&gt;&gt;5,870 hrs)
</p>
</div>
</div>

<div id="outline-container-orgd5587d7" class="outline-3">
<h3 id="orgd5587d7">Transcript</h3>
<div class="outline-text-3" id="text-orgd5587d7">
<p>
Hello.
Welcome to the Coursera course on Neural Networks for Machine
Learning.
</p>

<p>
Before we get into the details of neural network learning
algorithms, I want to talk a little bit about machine learning, why
we need machine learning, the kinds of things we use it for, and
show you some examples of what it can do. So the reason we need
machine learning is that the sum problem, where it's very hard to
write the programs, recognizing a three dimensional object for
example. When it's from a novel viewpoint and new lighting
additions in a cluttered scene is very hard to do. We don't know
what program to write because we don't know how it's done in our
brain. And even if we did know what program to write, it might be
that it was a horrendously complicated program. Another example is,
detecting a fraudulent credit card transaction, where there may not
be any nice, simple rules that will tell you it's fraudulent.
</p>

<p>
You really need to combine, a very large number of, not very
reliable rules. And also, those rules change every time because
people change the tricks they use for fraud. So, we need a
complicated program that combines unreliable rules, and that we can
change easily. The machine learning approach, is to say, instead of
writing each program by hand for each specific task, for particular
task, we collect a lot of examples, and specify the correct output
for given input. A machine learning algorithm then takes these
examples and produces a program that does the job. The program
produced by the linear algorithm may look very different from the
typical handwritten program. For example, it might contain millions
of numbers about how you weight different kinds of evidence. If we
do it right, the program should work for new cases just as well as
the ones it's trained on. And if the data changes, we should be
able to change the program runs very easily by retraining it on the
new data. And now massive amounts for computation are cheaper that
paying someone to write a program for a specific task, so we can
afford big complicated machine learning programs to produce these
stark task specific systems for us.
</p>

<p>
Some examples of the things that are best done by using a learning
algorithm are recognizing patterns, so for example objects in real
scenes, or the identities or expressions of people's faces, or
spoken words. There's also recognizing anomalies. So, an unusual
sequence of credit card transactions would be an anomaly. Another
example of an anomaly would be an unusual pattern of sensor
readings in a nuclear power plant. And you wouldn't really want to
have to deal with those by doing supervised learning. Where you
look at the ones that blow up, and see what, what caused them to
blow up. You'd really like to recognize that something funny is
happening without having any supervision signal. It's just not
behaving in its normal way. And then this prediction. So,
typically, predicting future stock prices or currency exchange
rates or predicting which movies a person will like from knowing
which other movies they like. And which movies a lot of other
people liked.
</p>

<p>
So in this course I'm mean as a standard example for explaining a
lot of the machine learning algorithms. This is done in a lot of
science. In genetics for example, a lot of genetics is done on
fruitflies. And the reason is they're convenient. They breed fast
and a lot is already known about the genetics of fruit flies. The
MNIST database of handwritten digits is the machine equivalent of
fruitflies. It's publicly available. We can get machine learning
algorithms to learn how to recognize these handwritten digits quite
quickly, so it's easy to try lots of variations. And we know huge
amounts about how well different machine learning methods do on
MNIST. And in particular, the different machine learning methods
were implemented by people who believed in them, so we can rely on
those results. So for all those reasons, we're gonna use MNIST as
our standard task.
</p>

<p>
Here's an example of some of the digits in MNIST. These are ones
that were correctly recognized by neural net the first time it saw
them. But the ones within the neural net wasn't very confident. And
you could see why. I've arranged these digits in standard scan line
order. So zeros, then ones, then twos and so on. If you look at a
bunch of tubes like the ounces in the green rectangle. You can see
that if you knew they were 100 in digit you'd probably guess they
were twos. But it's very hard to say what it is that makes them
twos. There is nothing simple that they all have in common. In
particular if you try and overlay one on another you'll see it
doesn't fit. And even if you skew it a bit, it's very hard to make
them overlay on each other.
</p>

<p>
So a template isn't going to do the job. An in particular template
is going to be very hard to find that will fit those twos in the
green box and would also fit the things in the red boxes. So that's
one thing that makes recognizing handwritten digits a good task for
machine learning.
</p>

<p>
Now, I don't want you to think that's the only thing we can do.
It's a relatively simple for our machine learning system to do now.
And to motivate the rest of the course, I want to show you some
examples of much more difficult things. So we now have neural nets
with approaching a hundred million parameters in them, that can
recognize a thousand different object classes in 1.3 million high
resolution training images got from the web. So, there was a
competition in 2010, and the best system got 47 percent error rate
if you look at its first choice, and 25 percent error rate if you
say it got it right if it was in its top five choices, which isn't
bad for 1,000 different objects.
</p>

<p>
Jitendra Malik who's an eminent neural net skeptic, and a leading
computer vision researcher, has said that this competition is a
good test of whether deep neural networks can work well for object
recognition. And a very deep neural network can now do considerably
better than the thing that won the competition. It can get less
than 40 percent error, for its first choice, and less than twenty
percent error for its top five choices. I'll describe that in much
more detail in lecture five.
</p>

<p>
Here's some examples of the kinds of images you have to recognize.
These images from the test set that he's never seen before. And
below the examples, I'm showing you what the neural net thought the
right answer was. Where the length of the horizontal bar is how
confident it was, and the correct answer is in red. So if you look
in the middle, it correctly identified that as a snow plow. But you
can see that its other choices are fairly sensible. It does look a
little bit like a drilling platform. And if you look at its third
choice, a lifeboat, it actually looks very like a lifeboat. You can
see the flag on the front of the boat and the bridge of the boat
and the flag at the back, and the high surf in the background. So
its, its errors tell you a lot about how it's doing it and they're
very plausible errors. If you look on the left, it gets it wrong
possibly because the beak of the bird is missing and cuz the
feathers of the bird look very like the wet fur of an otter. But it
gets it in its top five, and it does better than me. I wouldn't
know if that was a quail or a ruffed grouse or a partridge. If you
look on the right, it gets it completely wrong. It a guillotine,
you can why it says that. You can possibly see why it says
orangutan, because of the sort of jungle looking background and
something orange in the middle. But it fails to get the right
answer.
</p>

<p>
It can, however, deal with a wide range of different objects. If
you look on the left, I would have said microwave as my first
answer. The labels aren't very systematic. So actually, the correct
answer there is electric range. And it does get it in its top five.
In the middle, it's getting a turnstile, which is a distributed
object. It does, can't, it can do more than just recognize compact
things. And it can also deal with pictures, as well as real scenes,
like the bulletproof vest. And it makes some very cool errors. If
you look at the image on the left, that's an earphone. It doesn't
get anything, like an earphone. But if you look at this fourth
batch, it thinks it's an ant. And for you to think that's crazy.
But then if you look at it carefully, you can see it's a view of an
ant from underneath. The eyes are looking down at you, and you can
see the antennae behind it. It's not the kind of view of an ant
you'd like to have if you were a green fly. If you look at the one
on the right, it doesn't get the right answer. But all of its
answers are, cylindrical objects.
</p>

<p>
Another task that neural nets are now very good at, is speech
recognition. Or at least part of a speech recognition system. So
speech recognition systems have several stages. First they
pre-process the sound wave, to get a vector of acoustic
coefficients, for each ten milliseconds of sound wave. And so they
get 100 of those actors per second. They then take a few adjacent
vectors of acoustic coefficients, and they need to place bets on
which part of which phoneme is being spoken. So they look at this
little window and they say, in the middle of this window, what do I
think the phoneme is, and which part of the phoneme is it? And a
good speech recognition system will have many alternative models
for a phoneme. And each model, it might have three different parts.
So it might have many thousands of alternative fragments that it
thinks this might be. And you have to place bets on all those
thousands of alternatives. And then once you place those bets you
have a decoding stage that does the best job it can of using
plausible bets, but piecing them together into a sequence of bets
that corresponds to the kinds of things that people say. Currently,
deep neural networks pioneered by George Dahl and Abdel-rahman
Mohammed of the University of Toronto are doing better than
previous machine learning methods for the acoustic model, and
they're now beginning to be used in practical systems. So, Dahl and
Mohammed, developed a system, that uses many layers of, binary
neurons, to, take some acoustic frames, and make bets about the
labels. They were doing it on a fairly small database and then used
183 alternative labels. And to get their system to work well, they
did some pre-training, which will be described in the second half
of the course. After standard post processing, they got 20.7
percent error rate on a very standard benchmark, which is kind of
like the NMIST for speech. The best previous result on that
benchmark for speak independent recognition was 24.4%. And a very
experienced speech researcher at Microsoft research realized that,
that was a big enough improvement, that probably this would change
the way speech recognition systems were done. And indeed, it has.
So, if you look at recent results from several different leading
speech groups, Microsoft showed that this kind of deep neural
network, when used as the acoustic model in the speech system.
Reduced the error rate from 27.4 percent to 18.5%, or
alternatively, you could view it as reducing the amount of training
data you needed from 2,000 hours down to 309 hours to get
comparable performance. Ibm which has the best system for one of
the standard speech recognition tasks for large recovery speech
recognition, showed that even it's very highly tuned system that
was getting 18.8 percent can be beaten by one of these deep neural
networks. And Google, fairly recently, trained a deep neural
network on a large amount of speech, 5,800 hours. That was still
much less than they trained their mixture model on. But even with
much less data, it did a lot better than the technology they had
before. So it reduced the error rate from sixteen percent to 12.3
percent and the error rate is still falling. And in the latest
Android, if you do voice search, it's using one of these deep
neurall networks in order to do very good speech recognition.
</p>
</div>
</div>
</div>

<div id="outline-container-orge5bad48" class="outline-2">
<h2 id="orge5bad48">What are neural networks?</h2>
<div class="outline-text-2" id="text-orge5bad48">
</div><div id="outline-container-orge893fe5" class="outline-3">
<h3 id="orge893fe5">Reasons to study neural computation</h3>
<div class="outline-text-3" id="text-orge893fe5">
<p>
To understand how the brain actually works.
</p>

<ul class="org-ul">
<li>Its very big and very complicated and made of stuff that dies
when you poke it around. So we need to use computer simulations.</li>
</ul>
<p>
To understand a style of parallel computation inspired by neurons and their
adaptive connections.
</p>
<ul class="org-ul">
<li>Very different style from sequential computation.
<ul class="org-ul">
<li>Should be good for things that brains are good at (e.g. vision)</li>
<li>Shoud be bad for things that brains are bad at (e.g. 23 x 71)</li>
</ul></li>
</ul>
<p>
To solve practical problems by using novel learning algorithms inspired by
the brain (this course)
</p>
<ul class="org-ul">
<li>Learning algorithms can be very useful even if they are not how the</li>
</ul>
<p>
brain actually works.
</p>
</div>
</div>

<div id="outline-container-org4b90c43" class="outline-3">
<h3 id="org4b90c43">A typical cortical neuron</h3>
<div class="outline-text-3" id="text-org4b90c43">
<p>
Gross physical structure:
</p>
<ul class="org-ul">
<li>There is one axon that branches</li>
<li>There is a dendritic tree that collects input from other neurons.</li>
</ul>

<p>
Axons typically contact dendritic trees at synapses
</p>

<ul class="org-ul">
<li>A spike of activity in the axon causes charge to be injected into
the post-synaptic neuron.</li>
</ul>

<p>
Spike generation:
</p>
<ul class="org-ul">
<li>There is an <i>axon hillock</i> that generates outgoing spikes
whenever enough charge has flowed in at synapses to depolarize
the cell membrane.</li>
</ul>

<p>
axon
body
axon hillock
dendritic
tree
</p>
</div>
</div>

<div id="outline-container-orgae27dfa" class="outline-3">
<h3 id="orgae27dfa">Synapses</h3>
<div class="outline-text-3" id="text-orgae27dfa">
<p>
When a spike of activity travels along an axon and
arrives at a synapse it causes vesicles of transmitter
chemical to be released.
</p>

<ul class="org-ul">
<li>There are several kinds of transmitter.</li>
</ul>

<p>
The transmitter molecules diffuse across the synaptic
cleft and bind to receptor molecules in the membrane of
the post-synaptic neuron thus changing their shape.
</p>

<ul class="org-ul">
<li>This opens up holes that allow specific ions in or out.</li>
</ul>
</div>
</div>

<div id="outline-container-orgaf2a77c" class="outline-3">
<h3 id="orgaf2a77c">How synapses adapt</h3>
<div class="outline-text-3" id="text-orgaf2a77c">
<p>
The effectiveness of the synapse can be changed:
</p>

<ul class="org-ul">
<li>vary the number of vesicles of transmitter.</li>
<li>vary the number of receptor molecules.</li>
</ul>

<p>
Synapses are slow, but they have advantages over RAM
</p>

<ul class="org-ul">
<li>They are very small and very low-power.</li>
<li>They adapt using locally available signals
<ul class="org-ul">
<li>But what rules do they use to decide how to change?</li>
</ul></li>
</ul>
</div>
</div>

<div id="outline-container-orge9999d0" class="outline-3">
<h3 id="orge9999d0">How the brain works on one slide!</h3>
<div class="outline-text-3" id="text-orge9999d0">
<p>
Each neuron receives inputs from other neurons
</p>

<ul class="org-ul">
<li>A few neurons also connect to receptors.</li>
<li>Cortical neurons use spikes to communicate.</li>
</ul>

<p>
The effect of each input line on the neuron is controlled
by a synaptic weight
</p>

<ul class="org-ul">
<li>The weights can be positive or negative.</li>
</ul>

<p>
The synaptic weights <i>adapt</i> so that the whole network learns to
perform useful computations
</p>

<ul class="org-ul">
<li>Recognizing objects, understanding language, making plans,</li>
</ul>
<p>
controlling the body.
</p>

<p>
You have about 10^11 neurons each with about 10^4 weights.
</p>

<ul class="org-ul">
<li>A huge number of weights can affect the computation in a very
short time. Much better bandwidth than a workstation.</li>
</ul>
</div>
</div>

<div id="outline-container-org1c97488" class="outline-3">
<h3 id="org1c97488">Modularity and the brain</h3>
<div class="outline-text-3" id="text-org1c97488">
<p>
Different bits of the cortex do different things.
</p>

<ul class="org-ul">
<li>Local damage to the brain has specific effects.</li>
<li>Specific tasks increase the blood flow to specific regions.</li>
</ul>

<p>
But cortex looks pretty much the same all over.
</p>

<ul class="org-ul">
<li>Early brain damage makes functions relocate.</li>
</ul>

<p>
Cortex is made of general purpose stuff that has the ability to
turn into special purpose hardware in response to experience.
</p>

<ul class="org-ul">
<li>This gives rapid parallel computation plus flexibility.</li>
<li>Conventional computers get flexibility by having stored
sequential programs, but this requires very fast central
processors to perform long sequential computations.</li>
</ul>
</div>
</div>
</div>

<div id="outline-container-org5c54bce" class="outline-2">
<h2 id="org5c54bce">Some simple models of neurons</h2>
<div class="outline-text-2" id="text-org5c54bce">
</div><div id="outline-container-org80167bc" class="outline-3">
<h3 id="org80167bc">Idealized neurons</h3>
<div class="outline-text-3" id="text-org80167bc">
<p>
• 
• 
To model things we have to idealize them (e.g. atoms)
</p>
<ul class="org-ul">
<li>Idealization removes complicated details that are not essential</li>
</ul>
<p>
for understanding the main principles.
</p>
<ul class="org-ul">
<li>It allows us to apply mathematics and to make analogies to</li>
</ul>
<p>
other, familiar systems.
</p>
<ul class="org-ul">
<li>Once we understand the basic principles, its easy to add</li>
</ul>
<p>
complexity to make the model more faithful.
It is often worth understanding models that are known to be wrong
(but we must not forget that they are wrong!)
</p>
<ul class="org-ul">
<li>E.g. neurons that communicate real values rather than discrete</li>
</ul>
<p>
spikes of activity.Linear neurons
  These are simple but computationally limited
</p>
<ul class="org-ul">
<li>If we can make them learn we may get insight into more</li>
</ul>
<p>
complicated neurons.
i th input
bias
y = b + ∑ x i w i
output
i
index over
input connections
weight on
i th inputLinear neurons
  These are simple but computationally limited
</p>
<ul class="org-ul">
<li>If we can make them learn we may get insight into more</li>
</ul>
<p>
complicated neurons.
y = b + ∑ x i w i
i
y
0
0
b + ∑ x i w i
iBinary threshold neurons
1
  McCulloch-Pitts (1943): influenced Von Neumann.
</p>
<ul class="org-ul">
<li>First compute a weighted sum of the inputs.</li>
<li>Then send out a fixed size spike of activity if</li>
</ul>
<p>
the weighted sum exceeds a threshold.
</p>
<ul class="org-ul">
<li>McCulloch and Pitts thought that each spike</li>
</ul>
<p>
is like the truth value of a proposition and
each neuron combines truth values to
compute the truth value of another
proposition!
0
threshold
weighted inputBinary threshold neurons
  There are two equivalent ways to write the equations for
a binary threshold neuron:
z = b + ∑ x i w i
z = ∑ x i w i
i
y =
1 if
z ≥ θ
0 otherwise
i
θ = −b
y =
1 if
z ≥0
0 otherwiseRectified Linear Neurons
(sometimes called linear threshold neurons)
They compute a linear weighted sum of their inputs.
The output is a non-linear function of the total input.
z = b + ∑ x i w i
i
z if z &gt;0
y =
0 otherwise
y
0
zSigmoid neurons
  These give a real-valued
output that is a smooth and
bounded function of their
total input.
</p>
<ul class="org-ul">
<li>Typically they use the</li>
</ul>
<p>
logistic function
</p>
<ul class="org-ul">
<li>They have nice</li>
</ul>
<p>
derivatives which make
learning easy (see
lecture 3).
z = b + ∑ x i w i
y =
i
1
−z
1 + e
1
y
0.5
0
0
zStochastic binary neurons
  These use the same equations
as logistic units.
</p>
<ul class="org-ul">
<li>But they treat the output of</li>
</ul>
<p>
the logistic as the
probability of producing a
spike in a short time
window.
  We can do a similar trick for
rectified linear units:
</p>
<ul class="org-ul">
<li>The output is treated as the</li>
</ul>
<p>
Poisson rate for spikes.
z = b + ∑ x i w i
p(s = 1) =
1 + e
i
1
p 0.5
0
1
0
z
−zNeural Networks for Machine Learning
Lecture 1d
A simple example of learning
Geoffrey Hinton
with
Nitish Srivastava
Kevin SwerskyA very simple way to recognize handwritten shapes
  Consider a neural network with two
layers of neurons.
0 1 2 3 4 5 6 7 8 9
</p>
<ul class="org-ul">
<li>neurons in the top layer represent</li>
</ul>
<p>
known shapes.
</p>
<ul class="org-ul">
<li>neurons in the bottom layer</li>
</ul>
<p>
represent pixel intensities.
  A pixel gets to vote if it has ink on it.
</p>
<ul class="org-ul">
<li>Each inked pixel can vote for several</li>
</ul>
<p>
different shapes.
  The shape that gets the most votes wins.How to display the weights
1
2
3
4
5
6
7
8
9
0
The input
image
Give each output unit its own “map” of the input image and display the weight
coming from each pixel in the location of that pixel in the map.
Use a black or white blob with the area representing the magnitude of the weight
and the color representing the sign.How to learn the weights
1
2
3
4
5
6
7
8
9
0
The image
Show the network an image and increment the weights from active pixels
to the correct class.
Then decrement the weights from active pixels to whatever class the
network guesses.1
2
3
4
5
6
The image
7
8
9
01
2
3
4
5
6
The image
7
8
9
01
2
3
4
5
6
The image
7
8
9
01
2
3
4
5
6
The image
7
8
9
01
2
3
4
5
6
The image
7
8
9
0The learned weights
1
2
3
4
5
6
7
8
9
0
The image
The details of the learning algorithm will be explained in future lectures.Why the simple learning algorithm is insufficient
  A two layer network with a single winner in the top layer is
equivalent to having a rigid template for each shape.
</p>
<ul class="org-ul">
<li>The winner is the template that has the biggest overlap</li>
</ul>
<p>
with the ink.
  The ways in which hand-written digits vary are much too
complicated to be captured by simple template matches of
whole shapes.
</p>
<ul class="org-ul">
<li>To capture all the allowable variations of a digit we need</li>
</ul>
<p>
to learn the features that it is composed of.Examples of handwritten digits that can be recognized
correctly the first time they are seenNeural Networks for Machine Learning
Lecture 1e
Three types of learning
Geoffrey Hinton
with
Nitish Srivastava
Kevin SwerskyTypes of learning task
  Supervised learning
</p>
<ul class="org-ul">
<li>Learn to predict an output when given an input vector.</li>
</ul>
<p>
Reinforcement learning
</p>
<ul class="org-ul">
<li>Learn to select an action to maximize payoff.</li>
</ul>
<p>
Unsupervised learning
</p>
<ul class="org-ul">
<li>Discover a good internal representation of the input.Two types of supervised learning</li>
</ul>
<p>
  Each training case consists of an input vector x and a target output t.
  Regression: The target output is a real number or a whole vector of
real numbers.
</p>
<ul class="org-ul">
<li>The price of a stock in 6 months time.</li>
<li>The temperature at noon tomorrow.</li>
</ul>
<p>
Classification: The target output is a class label.
</p>
<ul class="org-ul">
<li>The simplest case is a choice between 1 and 0.</li>
<li>We can also have multiple alternative labels.How supervised learning typically works</li>
</ul>
<p>
We start by choosing a model-class: y = f (x;W)
</p>
<ul class="org-ul">
<li>A model-class, f, is a way of using some numerical</li>
</ul>
<p>
parameters, W, to map each input vector, x, into a predicted
output y.
  Learning usually means adjusting the parameters to reduce the
discrepancy between the target output, t, on each training case
and the actual output, y, produced by the model.
1
2
</p>
<ul class="org-ul">
<li>For regression, 2 (y − t) is often a sensible measure of the</li>
</ul>
<p>
discrepancy.
</p>
<ul class="org-ul">
<li>For classification there are other measures that are generally</li>
</ul>
<p>
more sensible (they also work better).Reinforcement learning
  In reinforcement learning, the output is an action or sequence of
actions and the only supervisory signal is an occasional scalar reward.
</p>
<ul class="org-ul">
<li>The goal in selecting each action is to maximize the expected sum</li>
</ul>
<p>
of the future rewards.
</p>
<ul class="org-ul">
<li>We usually use a discount factor for delayed rewards so that we</li>
</ul>
<p>
don’t have to look too far into the future.
  Reinforcement learning is difficult:
</p>
<ul class="org-ul">
<li>The rewards are typically delayed so its hard to know where we</li>
</ul>
<p>
went wrong (or right).
</p>
<ul class="org-ul">
<li>A scalar reward does not supply much information.</li>
</ul>
<p>
  This course cannot cover everything and reinforcement learning is one
of the important topics we will not cover.Unsupervised learning
  For about 40 years, unsupervised learning was largely ignored by the
machine learning community
</p>
<ul class="org-ul">
<li>Some widely used definitions of machine learning actually excluded it.</li>
<li>Many researchers thought that clustering was the only form of</li>
</ul>
<p>
unsupervised learning.
  It is hard to say what the aim of unsupervised learning is.
</p>
<ul class="org-ul">
<li>One major aim is to create an internal representation of the input that</li>
</ul>
<p>
is useful for subsequent supervised or reinforcement learning.
</p>
<ul class="org-ul">
<li>You can compute the distance to a surface by using the disparity</li>
</ul>
<p>
between two images. But you don’t want to learn to compute
disparities by stubbing your toe thousands of times.Other goals for unsupervised learning
  It provides a compact, low-dimensional representation of the input.
</p>
<ul class="org-ul">
<li>High-dimensional inputs typically live on or near a low-</li>
</ul>
<p>
dimensional manifold (or several such manifolds).
</p>
<ul class="org-ul">
<li>Principal Component Analysis is a widely used linear method</li>
</ul>
<p>
for finding a low-dimensional representation.
  It provides an economical high-dimensional representation of the
input in terms of learned features.
</p>
<ul class="org-ul">
<li>Binary features are economical.</li>
<li>So are real-valued features that are nearly all zero.</li>
</ul>
<p>
It finds sensible clusters in the input.
</p>
<ul class="org-ul">
<li>This is an example of a very sparse code in which only one of</li>
</ul>
<p>
the features is non-zero.
</p>
</div>
</div>
</div>
