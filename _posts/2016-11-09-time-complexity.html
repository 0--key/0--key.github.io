---
layout: post
title: The time complexity
tagline: " of an algorithm"
permalink: /algorithms/time-complexity.html
categories: [algorithms, literate programming]
tags: [computation, complexity, analysis]
---
<div id="table-of-contents">
<h2>Table of Contents</h2>
<div id="text-table-of-contents">
<ul>
<li><a href="#orgheadline1">Definition</a></li>
</ul>
</div>
</div>

<div id="outline-container-orgheadline1" class="outline-2">
<h2 id="orgheadline1">Definition</h2>
<div class="outline-text-2" id="text-orgheadline1">

<div class="figure">
<p><img src="https://upload.wikimedia.org/wikipedia/commons/thumb/7/7e/Comparison_computational_complexity.svg/250px-Comparison_computational_complexity.svg.png" alt="Time Complexity" title="Time Complexity proportion" align="right" />
</p>
<p><span class="figure-number">Figure 1:</span> Graphs of number of operations, N vs input size, n for common complexities, assuming a coefficient of 1</p>
</div>
<blockquote>
<p>
In computer science, the <i>time complexity of an algorithm</i> quantifies the
amount of time taken by an algorithm to run as a function of the length of
the string representing the input. The time complexity of an algorithm is
commonly expressed using <b>big O</b> notation, which excludes coefficients and
lower order terms. When expressed this way, the time complexity is said to
be described asymptotically, i.e., as the input size goes to infinity. For
example, if the time required by an algorithm on all inputs of size n is
at most 5n<sup>3</sup> + 3n for any n (bigger than some n<sub>0</sub>), the asymptotic time
complexity is O(n<sup>3</sup>).
</p>

<p>
Time complexity is commonly estimated by counting the number of elementary
operations performed by the algorithm, where an elementary operation takes
a fixed amount of time to perform. Thus, the amount of time taken and the
number of elementary operations performed by the algorithm differ by at
most a constant factor.
</p>
</blockquote>
</div>
</div>
