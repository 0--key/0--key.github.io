---
layout: post
title: Neural network
tagline: " architecture overview"
permalink: /machine-learning/neural-network/architecture/overview.html
categories: [machine learning, neural networks]
tags: [architecture, overview]
---

<div id="table-of-contents">
<h2>Table of Contents</h2>
<div id="text-table-of-contents">
<ul>
<li><a href="#org2d59ff5">An overview of the main types of neural network architecture</a>
<ul>
<li><a href="#org631dfa2">Feed-forward neural networks</a></li>
<li><a href="#org31300b4">Recurrent networks</a></li>
<li><a href="#org6ec7188">Recurrent neural networks for modeling sequences</a></li>
<li><a href="#org3e971f5">An example of what recurrent neural nets can now do</a></li>
<li><a href="#orgfe167c9">Symmetrically connected networks</a></li>
<li><a href="#org4c303a8">Symmetrically connected networks with hidden units</a></li>
</ul>
</li>
<li><a href="#orge1d6cc6">Perceptrons: The first generation of neural networks</a>
<ul>
<li><a href="#org5600437">The standard paradigm for statistical pattern recognition</a></li>
<li><a href="#org5905138">The history of perceptrons</a></li>
<li><a href="#org1221b85">Binary threshold neurons (decision units)</a></li>
<li><a href="#org37361f4">How to learn biases using the same rule as we use for learning weights</a></li>
<li><a href="#orgfacfb7e">The perceptron convergence procedure:</a></li>
</ul>
</li>
<li><a href="#orgde1f485">A geometrical view of perceptrons</a>
<ul>
<li><a href="#org58dc982">Warning!</a></li>
<li><a href="#org6731899">Weight-space</a></li>
<li><a href="#org7503e36">The cone of feasible solutions</a></li>
</ul>
</li>
<li><a href="#orgefc13a4">Why the learning works</a>
<ul>
<li><a href="#org0b3356a">Why the learning procedure works (first attempt)</a></li>
<li><a href="#org1dacd0b">Why the learning procedure works</a></li>
<li><a href="#org85a8675">Informal sketch of proof of convergence</a></li>
</ul>
</li>
<li><a href="#orga353e61">What perceptrons can’t do</a>
<ul>
<li><a href="#orgf786a4c">The limitations of Perceptrons</a></li>
<li><a href="#orgd5d0107">What binary threshold neurons cannot do</a></li>
<li><a href="#orga413a20">A geometric view of what binary threshold neurons cannot do</a></li>
<li><a href="#org017f3fa">Discriminating simple patterns under translation with wrap-around</a></li>
<li><a href="#org36d89fa">Sketch of a proof</a></li>
<li><a href="#orgf90cee5">Why this result is devastating for Perceptrons</a></li>
<li><a href="#orgae96e0b">Learning with hidden units</a></li>
</ul>
</li>
</ul>
</div>
</div>
<blockquote>
<p>
An overview of the main types of neural network architecture
</p>
</blockquote>

<div id="outline-container-org2d59ff5" class="outline-2">
<h2 id="org2d59ff5">An overview of the main types of neural network architecture</h2>
<div class="outline-text-2" id="text-org2d59ff5">
</div><div id="outline-container-org631dfa2" class="outline-3">
<h3 id="org631dfa2">Feed-forward neural networks</h3>
<div class="outline-text-3" id="text-org631dfa2">

<div class="figure">
<p><img src="https://upload.wikimedia.org/wikipedia/en/5/54/Feed_forward_neural_net.gif" alt="Feed-forward neural network" title="Feed-forward neural network" align="right" />
</p>
<p><span class="figure-number">Figure 1: </span>Feed-forward neural network</p>
</div>

<blockquote>
<p>
A feedforward neural network is an artificial neural network wherein
connections between the units do <i>not</i> form a cycle. As such, it is
different from recurrent neural networks.
</p>

<p>
The feedforward neural network was the first and simplest type of
artificial neural network devised. In this network, the information
moves in only one direction, forward, from the input nodes, through
the hidden nodes (if any) and to the output nodes. There are no cycles
or loops in the network.
</p>
</blockquote>

<p>
These are the commonest type of neural network in practical
applications.
</p>

<ul class="org-ul">
<li>The first layer is the input and the last layer is the output.</li>
<li>If there is more than one hidden layer, we call them “deep” neural
networks.</li>
</ul>

<p>
They compute a series of transformations that change the
similarities between cases.
</p>

<ul class="org-ul">
<li>The activities of the neurons in each layer are a non-linear
function of the activities in the layer below.</li>
</ul>
</div>
</div>

<div id="outline-container-org31300b4" class="outline-3">
<h3 id="org31300b4">Recurrent networks</h3>
<div class="outline-text-3" id="text-org31300b4">

<div class="figure">
<p><img src="http://cdn.information-management.com/media/newspics/021208_jones_fig2_M.gif" alt="Recurrent network" title="Recurrent network sample" align="right" />
</p>
<p><span class="figure-number">Figure 2: </span>Recurrent network</p>
</div>

<blockquote>
<p>
A recurrent neural network (RNN) is a class of artificial neural network
where connections between units form a directed cycle. This creates an
internal state of the network which allows it to exhibit dynamic
temporal behavior. Unlike feedforward neural networks, RNNs can use
their internal memory to process arbitrary sequences of inputs. This
makes them applicable to tasks such as unsegmented connected handwriting
recognition or speech recognition.
</p>
</blockquote>

<p>
These have directed cycles in their connection graph.
</p>

<ul class="org-ul">
<li>That means you can sometimes get back to where you started by
following the arrows. They can have complicated dynamics and this
can make them very difficult to train.</li>
<li>There is a lot of interest at present in finding efficient ways of
training recurrent nets. They are more biologically realistic.</li>
</ul>

<p>
Recurrent nets with multiple hidden layers are just a special case
that has some of the hidden &rarr; hidden connections missing.
</p>
</div>
</div>

<div id="outline-container-org6ec7188" class="outline-3">
<h3 id="org6ec7188">Recurrent neural networks for modeling sequences</h3>
<div class="outline-text-3" id="text-org6ec7188">
<p>
Recurrent neural networks are a very natural way to model sequential
data:
</p>

<ul class="org-ul">
<li>They are equivalent to very deep nets with one hidden layer per
time slice.</li>
<li>Except that they use the same weights at every time slice and they
get input at every time slice.</li>
</ul>

<p>
They have the ability to remember information in their hidden state
for a long time.
</p>

<ul class="org-ul">
<li>But its very hard to train them to use this potential.</li>
</ul>
</div>
</div>

<div id="outline-container-org3e971f5" class="outline-3">
<h3 id="org3e971f5">An example of what recurrent neural nets can now do</h3>
<div class="outline-text-3" id="text-org3e971f5">
<p>
(to whet your interest!)
</p>

<p>
Ilya Sutskever (2011) trained a special type of recurrent neural net
to predict the next character in a sequence.
</p>

<p>
After training for a long time on a string of half a billion
characters from English Wikipedia, he got it to generate new text.
</p>

<ul class="org-ul">
<li>It generates by predicting the probability distribution for the
next character and then sampling a character from this
distribution.</li>
<li>The next slide shows an example of the kind of text it generates.</li>
</ul>

<p>
Notice how much it knows!Some text generated one character at a time
by Ilya Sutskever’s recurrent neural network In 1974 Northern Denver
had been overshadowed by CNL, and several Irish intelligence agencies
in the Mediterranean region. However, on the Victoria, Kings Hebrew
stated that Charles decided to escape during an alliance. The mansion
house was completed in 1882, the second in its bridge are omitted,
while closing is the proton reticulum composed below it aims, such
that it is the blurring of appearing on any well-paid type of box
printer.
</p>
</div>
</div>

<div id="outline-container-orgfe167c9" class="outline-3">
<h3 id="orgfe167c9">Symmetrically connected networks</h3>
<div class="outline-text-3" id="text-orgfe167c9">
<p>
These are like recurrent networks, but the connections between units
are symmetrical (they have the same weight in both directions).
</p>

<ul class="org-ul">
<li>John Hopfield (and others) realized that symmetric networks are
much easier to analyze than recurrent networks.</li>
<li>They are also more restricted in what they can do. because they
obey an energy function.</li>
</ul>

<p>
For example, they cannot model cycles.
Symmetrically connected nets without hidden units are called
“Hopfield nets”.
</p>
</div>
</div>

<div id="outline-container-org4c303a8" class="outline-3">
<h3 id="org4c303a8">Symmetrically connected networks with hidden units</h3>
<div class="outline-text-3" id="text-org4c303a8">
<p>
These are called “Boltzmann machines”.
</p>
<ul class="org-ul">
<li>They are much more powerful models than Hopfield nets.</li>
<li>They are less powerful than recurrent neural networks.</li>
<li>They have a beautifully simple learning algorithm.</li>
</ul>

<p>
We will cover Boltzmann machines towards the end of the
course
</p>
</div>
</div>
</div>


<div id="outline-container-orge1d6cc6" class="outline-2">
<h2 id="orge1d6cc6">Perceptrons: The first generation of neural networks</h2>
<div class="outline-text-2" id="text-orge1d6cc6">
</div><div id="outline-container-org5600437" class="outline-3">
<h3 id="org5600437">The standard paradigm for statistical pattern recognition</h3>
<div class="outline-text-3" id="text-org5600437">

<div class="figure">
<p><img src="http://www.what2web.com/wp-content/uploads/2017/01/perceptron-architecture-300x251.jpg" alt="Standard perceptron architecture" title="simplifyed image" align="right" />
</p>
<p><span class="figure-number">Figure 3: </span>Standard perceptron architecture</p>
</div>

<ol class="org-ol">
<li>Convert the raw input vector into a vector of feature activations.</li>
</ol>
<p>
Use hand-written programs based on common-sense to define the
features.
</p>

<ol class="org-ol">
<li>Learn how to weight each of the feature activations to get a single</li>
</ol>
<p>
scalar quantity.
</p>

<ol class="org-ol">
<li>If this quantity is above some threshold, decide that the input</li>
</ol>
<p>
vector is a positive example of the target class.
</p>
</div>
</div>

<div id="outline-container-org5905138" class="outline-3">
<h3 id="org5905138">The history of perceptrons</h3>
<div class="outline-text-3" id="text-org5905138">
<p>
They were popularised by Frank Rosenblatt in the early 1960’s.
</p>

<ul class="org-ul">
<li>They appeared to have a very powerful learning algorithm.</li>
<li>Lots of grand claims were made for what they could learn to do.</li>
</ul>

<p>
In 1969, Minsky and Papert published a book called “Perceptrons”
that analysed what they could do and showed their limitations.
</p>

<ul class="org-ul">
<li>Many people thought these limitations applied to all neural network
models.</li>
</ul>

<p>
The perceptron learning procedure is still widely used today for tasks
with enormous feature vectors that contain many millions of features.
</p>
</div>
</div>

<div id="outline-container-org1221b85" class="outline-3">
<h3 id="org1221b85">Binary threshold neurons (decision units)</h3>
<div class="outline-text-3" id="text-org1221b85">
<p>
McCulloch-Pitts (1943)
</p>

<ul class="org-ul">
<li>First compute a weighted sum of the inputs from other neurons
(plus a bias).</li>
<li>Then output a 1 if the weighted sum exceeds zero.</li>
</ul>
</div>
</div>

<div id="outline-container-org37361f4" class="outline-3">
<h3 id="org37361f4">How to learn biases using the same rule as we use for learning weights</h3>
<div class="outline-text-3" id="text-org37361f4">
<p title="Binary Threshold Neuron – Training Biases" align="right">
<img src="http://www.what2web.com/wp-content/uploads/2017/01/binary-threshold-neuron-bias-training-300x255.jpg" title="Binary Threshold Neuron – Training Biases" align="right" />
A threshold is equivalent to having a negative bias. We can avoid
having to figure out a separate learning rule for the bias by using
a trick:
</p>

<ul class="org-ul">
<li>A bias is exactly equivalent to a weight on an extra input line
that always has an activity of 1.</li>
<li>We can now learn a bias as if it were a weight.</li>
</ul>
</div>
</div>

<div id="outline-container-orgfacfb7e" class="outline-3">
<h3 id="orgfacfb7e">The perceptron convergence procedure:</h3>
<div class="outline-text-3" id="text-orgfacfb7e">
<p>
Training binary output neurons as classifiers
Add an extra component with value 1 to each input vector. The “bias” weight
on this component is minus the threshold. Now we can forget the threshold.
Pick training cases using any policy that ensures that every training case will
keep getting picked.
</p>
<ul class="org-ul">
<li>If the output unit is correct, leave its weights alone.</li>
<li>If the output unit incorrectly outputs a zero, add the input
vector to the weight vector.</li>
<li>If the output unit incorrectly outputs a 1, subtract the input
vector from the weight vector.</li>
</ul>
<p>
This is guaranteed to find a set of weights that gets the right
answer for all the training cases if any such set exists.Neural
</p>
</div>
</div>
</div>


<div id="outline-container-orgde1f485" class="outline-2">
<h2 id="orgde1f485">A geometrical view of perceptrons</h2>
<div class="outline-text-2" id="text-orgde1f485">
</div><div id="outline-container-org58dc982" class="outline-3">
<h3 id="org58dc982">Warning!</h3>
<div class="outline-text-3" id="text-org58dc982">
<p>
For non-mathematicians, this is going to be tougher than the previous
material.
</p>

<ul class="org-ul">
<li>You may have to spend a long time studying the next two parts.</li>
</ul>

<p>
If you are not used to thinking about hyper-planes in high-dimensional
spaces, now is the time to learn.
To deal with hyper-planes in a 14-dimensional space, visualize a 3-D
space and say “fourteen” to yourself very loudly. Everyone does it.
</p>

<ul class="org-ul">
<li>But remember that going from 13-D to 14-D creates as much extra
complexity as going from 2-D to 3-D.</li>
</ul>
</div>
</div>

<div id="outline-container-org6731899" class="outline-3">
<h3 id="org6731899">Weight-space</h3>
<div class="outline-text-3" id="text-org6731899">

<div class="figure">
<p><img src="https://i.stack.imgur.com/nzHSl.jpg" alt="neural network" width="50%" height="50%" title="Neural network representation" align="right" />
</p>
<p><span class="figure-number">Figure 4: </span>Weight space</p>
</div>

<p>
This space has one dimension per weight.
</p>

<p>
A point in the space represents a particular setting of all the weights.
</p>

<p>
Assuming that we have eliminated the threshold, each training case
can be represented as a hyperplane through the origin.
</p>

<ul class="org-ul">
<li>The weights must lie on one side of this hyper-plane to get the
answer correct .Weight space</li>
</ul>
<p>
Each training case defines a plane (shown as a black line)
</p>
<ul class="org-ul">
<li>The plane goes through the origin and is perpendicular to the
input vector.</li>
<li>On one side of the plane the output is wrong because the scalar
product of the weight vector with the input vector has the wrong
sign.</li>
</ul>

<p>
Each training case defines a plane (shown as a black line)
</p>
<ul class="org-ul">
<li>The plane goes through the origin and is perpendicular to the
input vector.</li>
<li>On one side of the plane the output is wrong because the scalar
product of the weight vector with the input vector has the wrong
sign</li>
</ul>
</div>
</div>

<div id="outline-container-org7503e36" class="outline-3">
<h3 id="org7503e36">The cone of feasible solutions</h3>
<div class="outline-text-3" id="text-org7503e36">
<p>
To get all training cases right we need to find a point on the
right side of all the planes.
</p>

<ul class="org-ul">
<li>There may not be any such point!</li>
</ul>

<p>
If there are any weight vectors that get the right answer for all
cases, they lie in a hyper-cone with its apex at the origin.
</p>

<ul class="org-ul">
<li>So the average of two good weight vectors is a good weight
vector.</li>
</ul>

<p>
The problem is convex.
</p>
</div>
</div>
</div>


<div id="outline-container-orgefc13a4" class="outline-2">
<h2 id="orgefc13a4">Why the learning works</h2>
<div class="outline-text-2" id="text-orgefc13a4">
</div><div id="outline-container-org0b3356a" class="outline-3">
<h3 id="org0b3356a">Why the learning procedure works (first attempt)</h3>
<div class="outline-text-3" id="text-org0b3356a">
<p>
Consider the squared distance d<sub>a</sub><sup>2</sup> + d<sub>b</sub><sup>2</sup> between any feasible
weight vector and the current weight vector.
</p>

<ul class="org-ul">
<li><i>Hopeful claim</i>: Every time the perceptron makes a mistake, the</li>
</ul>
<p>
learning algorithm moves the current weight vector closer to all
feasible weight vectors.
</p>

<p>
Problem case: The weight vector may not get closer to this feasible
vector!
</p>
</div>
</div>

<div id="outline-container-org1dacd0b" class="outline-3">
<h3 id="org1dacd0b">Why the learning procedure works</h3>
<div class="outline-text-3" id="text-org1dacd0b">
<p>
So consider “generously feasible” weight vectors that lie within
the feasible region by a margin at least as great as the length of
the input vector that defines each constraint plane.
</p>

<ul class="org-ul">
<li>Every time the perceptron makes a mistake, the squared distance
to all of these generously feasible weight vectors is always
decreased by at least the squared length of the update vector.</li>
</ul>
</div>
</div>

<div id="outline-container-org85a8675" class="outline-3">
<h3 id="org85a8675">Informal sketch of proof of convergence</h3>
<div class="outline-text-3" id="text-org85a8675">
<p>
Each time the perceptron makes a mistake, the current weight vector
moves to decrease its squared distance from every weight vector in
the “generously feasible” region.
</p>

<p>
The squared distance decreases by at least the squared length of
the input vector.
</p>

<p>
So after a finite number of mistakes, the weight vector must lie in
the feasible region if this region exists.Neural Networks for
Machine Learning
</p>
</div>
</div>
</div>

<div id="outline-container-orga353e61" class="outline-2">
<h2 id="orga353e61">What perceptrons can’t do</h2>
<div class="outline-text-2" id="text-orga353e61">
</div><div id="outline-container-orgf786a4c" class="outline-3">
<h3 id="orgf786a4c">The limitations of Perceptrons</h3>
<div class="outline-text-3" id="text-orgf786a4c">
<p>
If you are allowed to choose the features by hand and if you use
enough features, you can do almost anything.
</p>
<ul class="org-ul">
<li>For binary input vectors, we can have a separate feature unit for
each of the exponentially many binary vectors and so we can make
any possible discrimination on binary input vectors.
<ul class="org-ul">
<li>This type of table look-up won’t generalize.</li>
</ul></li>
</ul>
<p>
But once the hand-coded features have been determined, there are
very strong limitations on what a perceptron can learn.
</p>
</div>
</div>

<div id="outline-container-orgd5d0107" class="outline-3">
<h3 id="orgd5d0107">What binary threshold neurons cannot do</h3>
<div class="outline-text-3" id="text-orgd5d0107">

<div class="figure">
<p><img src="http://0--key.github.io/assets/img/neural_networks/perceptron_representation.png" alt="Perceptron schema" width="30%" title="Graphical representation" align="right" />
</p>
<p><span class="figure-number">Figure 5: </span>Binary threshold neurons</p>
</div>

<p>
A binary threshold output unit cannot even tell if two single bit
features are the same!
</p>

<ul class="org-ul">
<li>Positive cases (same): (1,1) &rarr; 1; (0,0) &rarr; 1</li>
<li>Negative cases (different): (1,0) &rarr; 0; (0,1) &rarr; 0</li>
</ul>

<p>
The four input-output pairs give four inequalities that are impossible to
satisfy:
w<sub>1</sub> + w<sub>2</sub> ≥ θ ,
w<sub>1</sub> &lt; θ ,
0 ≥ θ
w<sub>2</sub> &lt; θ
</p>
</div>
</div>

<div id="outline-container-orga413a20" class="outline-3">
<h3 id="orga413a20">A geometric view of what binary threshold neurons cannot do</h3>
<div class="outline-text-3" id="text-orga413a20">

<div class="figure">
<p><img src="http://0--key.github.io/assets/img/neural_networks/perceptron_limitation.png" alt="Perceptron limitation" width="40%" title="Geometric view" align="right" />
</p>
<p><span class="figure-number">Figure 6: </span>Binary threshold neurons limits</p>
</div>

<p>
Imagine “data-space” in which the axes correspond to components of
an input vector.
</p>
<ul class="org-ul">
<li>Each input vector is a point in this space.</li>
<li>A weight vector defines a plane in data-space.</li>
<li>The weight plane is perpendicular to the weight vector and misses
the origin by a distance equal to the threshold.</li>
</ul>
</div>
</div>

<div id="outline-container-org017f3fa" class="outline-3">
<h3 id="org017f3fa">Discriminating simple patterns under translation with wrap-around</h3>
<div class="outline-text-3" id="text-org017f3fa">

<div class="figure">
<p><img src="http://0--key.github.io/assets/img/neural_networks/simple_patterns.png" alt="Perceptron limitation" width="30%" title="Sketch view" align="right" />
</p>
<p><span class="figure-number">Figure 7: </span>Simple patterns discrimination</p>
</div>

<p>
Suppose we just use pixels as the features. Can a binary threshold
unit discriminate between different patterns that have the same
number of on pixels?
</p>
<ul class="org-ul">
<li><b>Not</b> if the patterns can translate with wrap-around!</li>
</ul>
</div>
</div>

<div id="outline-container-org36d89fa" class="outline-3">
<h3 id="org36d89fa">Sketch of a proof</h3>
<div class="outline-text-3" id="text-org36d89fa">
<p>
that a binary decision unit cannot discriminate patterns with the
same number of on pixels (assuming translation with wraparound)
</p>

<p>
For pattern A, use training cases in all possible translations.
</p>
<ul class="org-ul">
<li>Each pixel will be activated by 4 different translations of pattern A.</li>
<li>So the total input received by the decision unit over all these
patterns will be four times the sum of all the weights.</li>
</ul>

<p>
For pattern B, use training cases in all possible translations.
</p>
<ul class="org-ul">
<li>Each pixel will be activated by 4 different translations of pattern B.</li>
<li>So the total input received by the decision unit over all these
patterns will be four times the sum of all the weights.</li>
</ul>

<p>
But to discriminate correctly, every single case of pattern A must
provide more input to the decision unit than every single case of
pattern B.
</p>
<ul class="org-ul">
<li>This is impossible if the sums over cases are the same.</li>
</ul>
</div>
</div>


<div id="outline-container-orgf90cee5" class="outline-3">
<h3 id="orgf90cee5">Why this result is devastating for Perceptrons</h3>
<div class="outline-text-3" id="text-orgf90cee5">
<p>
The whole point of pattern recognition is to recognize patterns
despite transformations like translation.
</p>

<p>
Minsky and Papert’s “Group Invariance Theorem” says that the part
of a Perceptron that learns cannot learn to do this if the
transformations form a group.
</p>

<ul class="org-ul">
<li>Translations with wrap-around form a group.</li>
</ul>

<p>
To deal with such transformations, a Perceptron needs to use
multiple feature units to recognize transformations of informative
sub-patterns.
</p>

<ul class="org-ul">
<li>So the tricky part of pattern recognition must be solved by the
hand-coded feature detectors, not the learning procedure.</li>
</ul>
</div>
</div>

<div id="outline-container-orgae96e0b" class="outline-3">
<h3 id="orgae96e0b">Learning with hidden units</h3>
<div class="outline-text-3" id="text-orgae96e0b">
<p>
Networks without hidden units are very limited in the input-output
mappings they can learn to model.
</p>

<ul class="org-ul">
<li>More layers of linear units do not help. Its still linear.</li>
<li>Fixed output non-linearities are not enough.</li>
</ul>

<p>
We need multiple layers of <b>adaptive</b>, non-linear hidden units. But
how can we train such nets?
</p>

<ul class="org-ul">
<li>We need an efficient way of <i>adapting all the weights</i>, not just
the last layer. This is hard.</li>
<li>Learning the weights going into hidden units is equivalent to
learning features.</li>
<li>This is difficult because nobody is telling us directly what the
hidden units should do.</li>
</ul>
</div>
</div>
</div>
